# Analisis_Sentimiento_IMDB_ModeloF1

üß≠ 1. Definici√≥n del problema
Objetivo: Construir un modelo que clasifique rese√±as de pel√≠culas como positivas o negativas.

M√©trica clave: F1-score ‚â• 0.85.

Tipo de problema: Clasificaci√≥n binaria de texto.

üì• 2. Carga y exploraci√≥n inicial de datos
Tareas:

Cargar el dataset proporcionado.

Verificar estructura, tipos de datos, valores nulos o duplicados.

Revisar ejemplos de rese√±as y distribuci√≥n de clases.

Herramientas: pandas, matplotlib, seaborn.

üìä 3. An√°lisis exploratorio de datos (EDA)
Tareas:

Visualizar la distribuci√≥n de clases (positivas vs. negativas).

Analizar longitud promedio de rese√±as, frecuencia de palabras, etc.

Concluir si existe desequilibrio de clases.

Herramientas: seaborn, nltk, wordcloud.

üßπ 4. Preprocesamiento de texto
Tareas:

Limpiar texto: min√∫sculas, eliminaci√≥n de signos, stopwords, etc.

Tokenizaci√≥n y lematizaci√≥n (si no se usa BERT).

Convertir texto a vectores:

Opci√≥n 1: TF-IDF

Opci√≥n 2: Word embeddings (Word2Vec, GloVe)

Opci√≥n 3: BERT embeddings (BERT_text_to_embeddings())

Herramientas: scikit-learn, nltk, transformers, spaCy.

ü§ñ 5. Entrenamiento de modelos
Tareas:

Dividir datos en entrenamiento y prueba.

Entrenar al menos tres modelos:

LogisticRegression

GradientBoostingClassifier o XGBoost

RandomForestClassifier o SVM

Evaluar con evaluate_model() y comparar F1-score y AUC-ROC.

Herramientas: scikit-learn, xgboost, lightgbm.

üß™ 6. Prueba con rese√±as personalizadas
Tareas:

Escribir rese√±as positivas y negativas.

Clasificarlas con los tres modelos entrenados.

Comparar predicciones y justificar diferencias (por ejemplo, sensibilidad al vocabulario o longitud del texto).

üìå 7. An√°lisis comparativo y hallazgos
Tareas:

Comparar m√©tricas de los modelos (F1, AUC, precisi√≥n, recall).

Analizar errores comunes (falsos positivos/negativos).

Justificar por qu√© un modelo supera a los dem√°s.

Evaluar si se cumple el umbral de F1 ‚â• 0.85.
